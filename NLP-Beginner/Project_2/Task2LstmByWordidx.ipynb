{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.vector_cache', 'TextClassification2.py', '未命名1.ipynb', '.ipynb_checkpoints', 'Task2LstmByIdf.ipynb', 'Task2LstmByWordidx.ipynb', 'Task2TorchText.ipynb', 'train.tsv', 'TextClassification.py', 'test.tsv', 'Task1.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import unicodedata, re, string\n",
    "import nltk\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "print(os.listdir(\"/home/xiyu/data/trainee/ZhuYanru\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/home/xiyu/data/trainee/ZhuYanru/train.tsv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(\"/home/xiyu/data/trainee/ZhuYanru/test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    #print(len(words))\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    #print(len(new_words))\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "#    words = remove_stopwords(words)\n",
    "    return words\n",
    "def convert_to_onehot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, series, of, escapades, demonstrating, the,...\n",
       "1    [a, series, of, escapades, demonstrating, the,...\n",
       "2                                          [a, series]\n",
       "3                                                  [a]\n",
       "4                                             [series]\n",
       "Name: Words, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Second step - passing through prep functions\n",
    "df_train['Words'] = df_train['Words'].apply(normalize) \n",
    "df_train['Words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16209\n",
      "16209\n"
     ]
    }
   ],
   "source": [
    "# Third step - creating a list of unique words to be used as dictionary for encoding\n",
    "word_set = set()\n",
    "for l in df_train['Words']:\n",
    "    for e in l:\n",
    "        word_set.add(e)\n",
    "        \n",
    "word_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n",
    "\n",
    "# Check if they are still the same lenght\n",
    "print(len(word_set))\n",
    "print(len(word_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [10180, 2043, 10983, 7728, 13131, 6354, 2022, ...\n",
       "1    [10180, 2043, 10983, 7728, 13131, 6354, 2022, ...\n",
       "2                                        [10180, 2043]\n",
       "3                                              [10180]\n",
       "4                                               [2043]\n",
       "Name: Tokens, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the dict to tokenize each phrase\n",
    "df_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\n",
    "df_train['Tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# Step four - get the len of longest phrase\n",
    "max_len = df_train['Tokens'].str.len().max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10180  2043 10983  7728 13131  6354  2022 13635  7534  9301  2633  2604\n",
      "   6354 13469  9301  8101  2633  2604  6354 12589 15478 10983  6088  2441\n",
      "  15665  6264  1608 10983  6088 10701 13506  4481 10983 10180  1825     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [10180  2043 10983  7728 13131  6354  2022 13635  7534  9301  2633  2604\n",
      "   6354 13469     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [10180  2043     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Pad each phrase representation with zeroes, starting from the beginning of sequence\n",
    "# Will use a combined list of phrases as np array for further work. This is expected format for the Pytorch utils to be used later\n",
    "\n",
    "all_tokens = np.array([t for t in df_train['Tokens']])\n",
    "encoded_labels = np.array([l for l in df_train['Sentiment']])\n",
    "\n",
    "# Create blank rows\n",
    "features = np.zeros((len(all_tokens), max_len), dtype=int)\n",
    "# for each phrase, add zeros at the end \n",
    "for i, row in enumerate(all_tokens):\n",
    "    features[i, :len(row)] = row\n",
    "\n",
    "#print first 3 values of the feature matrix \n",
    "print(features[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(124848, 48) \n",
      "Validation set: \t(15606, 48) \n",
      "Test set: \t\t(15606, 48)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of  resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312\n",
      "289\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "# create Tensor datasets\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 54\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Check the size of the loaders (how many batches inside)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # linear\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x.squeeze(1).long())\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # transform lstm output to input size of linear layers\n",
    "        lstm_out = lstm_out.transpose(0,1)\n",
    "        lstm_out = lstm_out[-1]\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)        \n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(16210, 1000)\n",
      "  (lstm): LSTM(1000, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(word_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 5\n",
    "embedding_dim = 1000\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 1.227470... Val Loss: 1.309456\n",
      "Epoch: 1/3... Step: 200... Loss: 1.305858... Val Loss: 1.300361\n",
      "Epoch: 1/3... Step: 300... Loss: 1.431606... Val Loss: 1.255270\n",
      "Epoch: 1/3... Step: 400... Loss: 1.340757... Val Loss: 1.246746\n",
      "Epoch: 1/3... Step: 500... Loss: 1.245960... Val Loss: 1.240999\n",
      "Epoch: 1/3... Step: 600... Loss: 0.963754... Val Loss: 1.233595\n",
      "Epoch: 1/3... Step: 700... Loss: 1.064288... Val Loss: 1.218975\n",
      "Epoch: 1/3... Step: 800... Loss: 1.023869... Val Loss: 1.204438\n",
      "Epoch: 1/3... Step: 900... Loss: 1.108044... Val Loss: 1.199867\n",
      "Epoch: 1/3... Step: 1000... Loss: 1.064528... Val Loss: 1.190405\n",
      "Epoch: 1/3... Step: 1100... Loss: 1.212808... Val Loss: 1.175635\n",
      "Epoch: 1/3... Step: 1200... Loss: 1.258247... Val Loss: 1.171855\n",
      "Epoch: 1/3... Step: 1300... Loss: 1.255235... Val Loss: 1.150404\n",
      "Epoch: 1/3... Step: 1400... Loss: 1.190359... Val Loss: 1.166017\n",
      "Epoch: 1/3... Step: 1500... Loss: 1.158703... Val Loss: 1.130741\n",
      "Epoch: 1/3... Step: 1600... Loss: 1.044975... Val Loss: 1.127075\n",
      "Epoch: 1/3... Step: 1700... Loss: 0.958378... Val Loss: 1.113446\n",
      "Epoch: 1/3... Step: 1800... Loss: 1.107178... Val Loss: 1.106739\n",
      "Epoch: 1/3... Step: 1900... Loss: 0.926812... Val Loss: 1.103481\n",
      "Epoch: 1/3... Step: 2000... Loss: 0.987038... Val Loss: 1.079444\n",
      "Epoch: 1/3... Step: 2100... Loss: 0.933750... Val Loss: 1.088476\n",
      "Epoch: 1/3... Step: 2200... Loss: 1.165410... Val Loss: 1.091165\n",
      "Epoch: 1/3... Step: 2300... Loss: 0.933854... Val Loss: 1.083702\n",
      "Epoch: 2/3... Step: 2400... Loss: 0.785115... Val Loss: 1.070766\n",
      "Epoch: 2/3... Step: 2500... Loss: 0.999568... Val Loss: 1.061887\n",
      "Epoch: 2/3... Step: 2600... Loss: 1.071825... Val Loss: 1.049565\n",
      "Epoch: 2/3... Step: 2700... Loss: 0.789907... Val Loss: 1.058322\n",
      "Epoch: 2/3... Step: 2800... Loss: 0.834572... Val Loss: 1.061082\n",
      "Epoch: 2/3... Step: 2900... Loss: 0.607118... Val Loss: 1.074532\n",
      "Epoch: 2/3... Step: 3000... Loss: 0.680609... Val Loss: 1.036924\n",
      "Epoch: 2/3... Step: 3100... Loss: 0.584251... Val Loss: 1.049219\n",
      "Epoch: 2/3... Step: 3200... Loss: 1.021722... Val Loss: 1.041405\n",
      "Epoch: 2/3... Step: 3300... Loss: 0.749634... Val Loss: 1.045942\n",
      "Epoch: 2/3... Step: 3400... Loss: 0.578900... Val Loss: 1.051297\n",
      "Epoch: 2/3... Step: 3500... Loss: 0.934297... Val Loss: 1.044328\n",
      "Epoch: 2/3... Step: 3600... Loss: 0.826129... Val Loss: 1.038428\n",
      "Epoch: 2/3... Step: 3700... Loss: 0.682586... Val Loss: 1.023047\n",
      "Epoch: 2/3... Step: 3800... Loss: 0.732922... Val Loss: 1.032285\n",
      "Epoch: 2/3... Step: 3900... Loss: 0.744314... Val Loss: 1.028830\n",
      "Epoch: 2/3... Step: 4000... Loss: 0.830924... Val Loss: 1.034379\n",
      "Epoch: 2/3... Step: 4100... Loss: 0.763893... Val Loss: 1.034961\n",
      "Epoch: 2/3... Step: 4200... Loss: 0.845411... Val Loss: 1.025424\n",
      "Epoch: 2/3... Step: 4300... Loss: 0.761174... Val Loss: 1.032764\n",
      "Epoch: 2/3... Step: 4400... Loss: 0.712142... Val Loss: 1.028955\n",
      "Epoch: 2/3... Step: 4500... Loss: 0.699900... Val Loss: 1.024278\n",
      "Epoch: 2/3... Step: 4600... Loss: 0.826220... Val Loss: 1.027668\n",
      "Epoch: 3/3... Step: 4700... Loss: 0.819150... Val Loss: 1.071434\n",
      "Epoch: 3/3... Step: 4800... Loss: 0.728792... Val Loss: 1.048127\n",
      "Epoch: 3/3... Step: 4900... Loss: 0.719425... Val Loss: 1.047761\n",
      "Epoch: 3/3... Step: 5000... Loss: 0.714535... Val Loss: 1.082677\n",
      "Epoch: 3/3... Step: 5100... Loss: 0.691838... Val Loss: 1.059131\n",
      "Epoch: 3/3... Step: 5200... Loss: 0.570560... Val Loss: 1.034889\n",
      "Epoch: 3/3... Step: 5300... Loss: 0.734852... Val Loss: 1.052681\n",
      "Epoch: 3/3... Step: 5400... Loss: 0.726757... Val Loss: 1.053175\n",
      "Epoch: 3/3... Step: 5500... Loss: 0.770426... Val Loss: 1.058355\n",
      "Epoch: 3/3... Step: 5600... Loss: 0.794768... Val Loss: 1.038930\n",
      "Epoch: 3/3... Step: 5700... Loss: 0.610948... Val Loss: 1.057291\n",
      "Epoch: 3/3... Step: 5800... Loss: 0.704357... Val Loss: 1.055154\n",
      "Epoch: 3/3... Step: 5900... Loss: 0.794389... Val Loss: 1.037810\n",
      "Epoch: 3/3... Step: 6000... Loss: 0.530822... Val Loss: 1.038244\n",
      "Epoch: 3/3... Step: 6100... Loss: 0.738248... Val Loss: 1.050791\n",
      "Epoch: 3/3... Step: 6200... Loss: 0.835820... Val Loss: 1.060783\n",
      "Epoch: 3/3... Step: 6300... Loss: 0.807774... Val Loss: 1.069376\n",
      "Epoch: 3/3... Step: 6400... Loss: 0.925977... Val Loss: 1.026151\n",
      "Epoch: 3/3... Step: 6500... Loss: 0.701156... Val Loss: 1.040503\n",
      "Epoch: 3/3... Step: 6600... Loss: 0.813469... Val Loss: 1.033984\n",
      "Epoch: 3/3... Step: 6700... Loss: 0.776912... Val Loss: 1.037886\n",
      "Epoch: 3/3... Step: 6800... Loss: 0.727237... Val Loss: 1.028638\n",
      "Epoch: 3/3... Step: 6900... Loss: 0.983878... Val Loss: 1.042049\n"
     ]
    }
   ],
   "source": [
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            #print(inputs)\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate the loss and perform backprop\n",
    "        #print(labels.long())\n",
    "        loss = criterion(output, labels.long())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output, labels.long())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.082\n",
      "Test accuracy: 0.575\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output,1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
